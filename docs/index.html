<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Sparse Attention Hub - Benchmark Results</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .header {
            background: rgba(255, 255, 255, 0.95);
            backdrop-filter: blur(10px);
            border-radius: 15px;
            padding: 30px;
            margin-bottom: 30px;
            box-shadow: 0 8px 32px rgba(0, 0, 0, 0.1);
        }
        
        h1 {
            color: #2c3e50;
            font-size: 2.5em;
            margin-bottom: 10px;
            text-align: center;
            background: linear-gradient(45deg, #667eea, #764ba2);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }
        
        .subtitle {
            text-align: center;
            color: #7f8c8d;
            font-size: 1.2em;
            margin-bottom: 20px;
        }
        
        .section {
            background: rgba(255, 255, 255, 0.95);
            backdrop-filter: blur(10px);
            border-radius: 15px;
            padding: 30px;
            margin: 20px 0;
            box-shadow: 0 8px 32px rgba(0, 0, 0, 0.1);
            transition: transform 0.3s ease;
        }
        
        .section:hover {
            transform: translateY(-5px);
        }
        
        h2 {
            color: #34495e;
            font-size: 1.8em;
            margin-bottom: 20px;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
        }
        
        h3 {
            color: #2c3e50;
            font-size: 1.4em;
            margin: 25px 0 15px 0;
        }
        
        .description {
            margin: 15px 0;
            color: #555;
            font-size: 1.1em;
        }
        
        .plot-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }
        
        .plot-link {
            display: block;
            padding: 20px;
            background: linear-gradient(135deg, #3498db, #2980b9);
            color: white;
            text-decoration: none;
            border-radius: 10px;
            text-align: center;
            font-weight: bold;
            transition: all 0.3s ease;
            box-shadow: 0 4px 15px rgba(52, 152, 219, 0.3);
        }
        
        .plot-link:hover {
            transform: translateY(-3px);
            box-shadow: 0 8px 25px rgba(52, 152, 219, 0.4);
            background: linear-gradient(135deg, #2980b9, #1f5f8b);
        }
        
        .data-files {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
        }
        
        .data-files ul {
            list-style: none;
            padding: 0;
        }
        
        .data-files li {
            margin: 10px 0;
            padding: 10px;
            background: white;
            border-radius: 5px;
            border-left: 4px solid #3498db;
        }
        
        .data-files a {
            color: #3498db;
            text-decoration: none;
            font-weight: bold;
        }
        
        .data-files a:hover {
            text-decoration: underline;
        }
        
        .repo-info {
            text-align: center;
            padding: 20px;
            background: linear-gradient(135deg, #e74c3c, #c0392b);
            color: white;
            border-radius: 10px;
            margin: 20px 0;
        }
        
        .repo-info a {
            color: white;
            text-decoration: underline;
        }
        
        .loading-note {
            background: #fff3cd;
            border: 1px solid #ffeaa7;
            border-radius: 10px;
            padding: 15px;
            margin: 20px 0;
            color: #856404;
        }
        
        @media (max-width: 768px) {
            .container {
                padding: 10px;
            }
            
            h1 {
                font-size: 2em;
            }
            
            .plot-grid {
                grid-template-columns: 1fr;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>Sparse Attention Hub</h1>
            <p class="subtitle">Benchmark Results & Interactive Visualizations</p>
        </div>
        
        <div class="section">
            <h2>About This Repository</h2>
            <div class="description">
                <p>This repository contains comprehensive benchmark results and interactive visualizations from the Sparse Attention Hub research project. 
                These results demonstrate the performance of various sparse attention mechanisms across different tasks and datasets, providing insights 
                into the effectiveness of different attention patterns in transformer models.</p>
                
                <p>The visualizations are interactive and allow researchers to explore the relationships between different configurations, 
                performance metrics, and attention patterns in detail.</p>
            </div>
        </div>

        <div class="section">
            <h2>Interactive Visualizations</h2>
            
            <h3>Loogle Long Dependency Summarization</h3>
            <div class="description">
                <p>Analysis of sparse attention performance on long dependency summarization tasks using the Loogle dataset. 
                These visualizations show how different sparse attention configurations affect the model's ability to handle 
                long-range dependencies in summarization tasks.</p>
            </div>
            
            <div class="plot-grid">
                <a href="micro_tests/plots/loogle_longdep_summarization/config_comparison.html" class="plot-link">
                    üìä Config Comparison
                </a>
                <a href="micro_tests/plots/loogle_longdep_summarization/config_type_analysis.html" class="plot-link">
                    üîç Config Type Analysis
                </a>
                <a href="micro_tests/plots/loogle_longdep_summarization/error_vs_density.html" class="plot-link">
                    üìà Error vs Density
                </a>
                <a href="micro_tests/plots/loogle_longdep_summarization/layer_analysis.html" class="plot-link">
                    üèóÔ∏è Layer Analysis
                </a>
            </div>
            
            <div class="loading-note">
                <strong>Note:</strong> Some visualizations are large (4-17MB) and may take a moment to load. Please be patient while the interactive plots load.
            </div>
            
            <h3>LongBench Passage Retrieval</h3>
            <div class="description">
                <p>Performance analysis on passage retrieval tasks using the LongBench dataset. These results show how sparse attention 
                mechanisms perform on information retrieval tasks with long contexts.</p>
            </div>
            
            <div class="plot-grid">
                <a href="micro_tests/plots/longbench_passage_retrieval_en/config_comparison.html" class="plot-link">
                    üìä Config Comparison
                </a>
                <a href="micro_tests/plots/longbench_passage_retrieval_en/config_type_analysis.html" class="plot-link">
                    üîç Config Type Analysis
                </a>
                <a href="micro_tests/plots/longbench_passage_retrieval_en/error_vs_density.html" class="plot-link">
                    üìà Error vs Density
                </a>
                <a href="micro_tests/plots/longbench_passage_retrieval_en/layer_analysis.html" class="plot-link">
                    üèóÔ∏è Layer Analysis
                </a>
            </div>
        </div>

        <div class="section">
            <h2>Data Files</h2>
            <p>Raw data files are available for further analysis and research:</p>
            
            <div class="data-files">
                <ul>
                    <li>
                        <a href="micro_tests/metadata.tsv">üìã metadata.tsv</a> 
                        <br><small>Benchmark metadata and configuration information (4.9MB)</small>
                    </li>
                    <li>
                        <a href="micro_tests/vector.tsv">üìä vector.tsv</a> 
                        <br><small>Vector representations and embeddings (1.8MB)</small>
                    </li>
                </ul>
            </div>
        </div>

        <div class="section">
            <h2>Research Context</h2>
            <div class="description">
                <p>Sparse attention mechanisms are a key area of research in transformer architecture optimization. By reducing the computational 
                complexity of attention from O(n¬≤) to O(n log n) or better, these methods enable processing of much longer sequences while 
                maintaining model performance.</p>
                
                <p>This repository provides empirical evidence of how different sparse attention patterns affect model performance across 
                various tasks, helping researchers understand the trade-offs between computational efficiency and model effectiveness.</p>
            </div>
        </div>

        <div class="repo-info">
            <h3>Main Repository</h3>
            <p>For more information about the Sparse Attention Hub project, including the full codebase, documentation, and additional research, 
            visit the main repository: <a href="https://github.com/[username]/sparse-attention-hub">sparse-attention-hub</a></p>
        </div>
    </div>
</body>
</html> 